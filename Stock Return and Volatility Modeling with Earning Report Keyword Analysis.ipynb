{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35e148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from math import prod\n",
    "from ast import literal_eval\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 40)\n",
    "pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde3f47",
   "metadata": {},
   "source": [
    "# 1. Classification of Keywords\n",
    "\n",
    "In this section, I create an excel file with brand new columns for each Category in the latter section of the Data spreadsheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e67d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files\n",
    "\n",
    "txt_files = glob.glob(os.path.join(\"Daily_Return Data\", \"*.txt\"))\n",
    "df_data_temp = pd.read_excel(open('Data.xlsx', 'rb'))\n",
    "\n",
    "# Filter irrelevant data as instructed\n",
    "\n",
    "df_data = df_data_temp.copy()\n",
    "df_data = df_data[df_data['documentType'] == 'EARNINGS_CALL']\n",
    "df_data = df_data[df_data['country'] == 'US']\n",
    "\n",
    "# Forward fill any missing identifiers by referencing earnings calls with the same id\n",
    "# Remove event columns as instructed\n",
    "# Populate nan keydriver cells with neutral score of 0\n",
    "# Populate nan Method2 Category cells with neutral scores of [0,0]\n",
    "\n",
    "df_data = df_data.sort_values(by=['companyId', 'mainIdentifier isin'])\n",
    "df_data['mainIdentifier isin'] = df_data.groupby(['companyId', 'companyName'])['mainIdentifier isin'].transform(lambda x: x.ffill())\n",
    "\n",
    "event_col = [col for col in df_data.columns if 'eventcount' in col.lower()]\n",
    "df_data = df_data.drop(columns = event_col)\n",
    "\n",
    "score_col = [col for col in df_data.columns if 'score' in col.lower()]\n",
    "df_data[score_col] = df_data[score_col].fillna(0)\n",
    "\n",
    "df_data.iloc[:, 205:] = df_data.iloc[:, 205:].fillna('[0, 0]')\n",
    "sorted_m2 = sorted(list(df_data.iloc[:, 205:].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95792348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Method2 Category by prefix\n",
    "# Calculate the aggregate [positive,negative] score for each Category in a dynamically created column\n",
    "\n",
    "def sum_tuple_group(row, cols):\n",
    "    total_first = 0\n",
    "    total_second = 0\n",
    "    for col in cols:\n",
    "        val = row[col]\n",
    "        if isinstance(val, str):\n",
    "            try:\n",
    "                val = literal_eval(val)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        total_first += val[0]\n",
    "        total_second += val[1]\n",
    "    return [total_first, total_second]\n",
    "\n",
    "\n",
    "grouped_cols = defaultdict(list)\n",
    "for col in sorted_m2:\n",
    "    prefix = col.split(\" - \")[0]\n",
    "    grouped_cols[prefix].append(col)\n",
    "\n",
    "for prefix, cols in grouped_cols.items():\n",
    "    new_col_name = f\"{prefix}_Total\"\n",
    "    df_data[new_col_name] = df_data.apply(lambda row: sum_tuple_group(row, cols), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07799176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export result as csv or xlsx\n",
    "\n",
    "df_data.to_csv('question1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e84ce",
   "metadata": {},
   "source": [
    "# 2. Modeling forward stock returns with keyDriver scores\n",
    "\n",
    "Here, I merge the earning calls table with the returns table, and calculate the forward returns after each earning call. \n",
    "\n",
    "After doing feature selection, I evaluate the performance of several traditional machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067dd1d",
   "metadata": {},
   "source": [
    "- Reprocess the earning calls dataframe in case someone wants to start here\n",
    "- Filter unnecessary documents and regions, fill in missing mainIdentifier isin\n",
    "- Remove events, populate missing scores\n",
    "- Add eventDate column which either will provide the same day as the earning call if the earning call is made before closing hours, or next day if the call is after hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b310282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data2 = df_data_temp.copy()\n",
    "df_data2 = df_data2[df_data2['documentType'] == 'EARNINGS_CALL']\n",
    "df_data2 = df_data2[df_data2['country'] == 'US']\n",
    "\n",
    "df_data2 = df_data2.sort_values(by=['companyId', 'mainIdentifier isin'])\n",
    "df_data2['mainIdentifier isin'] = df_data2.groupby(['companyId', 'companyName'])['mainIdentifier isin'].transform(lambda x: x.ffill())\n",
    "df_data2 = df_data2.rename(columns={'mainIdentifier isin': 'fsym_isin'})\n",
    "\n",
    "event_col = [col for col in df_data2.columns if 'eventcount' in col.lower()]\n",
    "df_data2 = df_data2.drop(columns = event_col)\n",
    "\n",
    "score_col = [col for col in df_data2.columns if 'score' in col.lower()]\n",
    "df_data2[score_col] = df_data2[score_col].fillna(0)\n",
    "\n",
    "def roll_date(timestamp):\n",
    "    dt = datetime.fromisoformat(timestamp)\n",
    "    cutoff_time = dt.replace(hour=16, minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    if dt >= cutoff_time:\n",
    "        dt += timedelta(days=1)\n",
    "    \n",
    "    return dt.date().isoformat()\n",
    "\n",
    "\n",
    "df_data2['eventDate'] = df_data2['eventTime'].apply(roll_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa8ba0a",
   "metadata": {},
   "source": [
    "- Upload the returns dataset, and reorganize with pivot for each row to have its own mainIdentifier isin\n",
    "- Merge the earnings and returns datasets where both returns are provided and there also a matching earning call of the same identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ef1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in txt_files:\n",
    "    df = pd.read_csv(file, delimiter=\"\\t\", header=None, encoding=\"utf-8\")\n",
    "    df_list.append(df)\n",
    "\n",
    "df_ret = pd.concat(df_list, ignore_index=True)\n",
    "df_filter = df_ret[df_ret[1] != 'DATE']\n",
    "df_returns = df_filter.rename(columns={0: \"BENCHMARK_ID\",\n",
    "                                       1: \"DATE\", \n",
    "                                       2: \"SECURITY_ID\",\n",
    "                                       3: \"Weight\",\n",
    "                                       4: \"p_price_returns\",\n",
    "                                       5: \"gd_class_gics_h\",\n",
    "                                       6: \"fsym_security_perm_id\",\n",
    "                                       7: \"p_symbol\",\n",
    "                                       8: \"fsym_isin\"})\n",
    "\n",
    "\n",
    "df_returns = df_returns.sort_values(by=['fsym_isin','DATE'])\n",
    "df_retwide = df_returns.pivot_table(index = ['fsym_isin'],\n",
    "                           columns = 'DATE',\n",
    "                           values = 'p_price_returns')\n",
    "\n",
    "df_retwide = df_retwide.fillna(1)\n",
    "\n",
    "df_merge = pd.merge(df_data2, df_retwide, on=['fsym_isin'], how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc6162",
   "metadata": {},
   "source": [
    "- With the eventDate provided earlier as the starting point, calculate several ranges of forward returns for each earning call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "797f75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneday_ret(row):\n",
    "    event_date = row['eventDate']\n",
    "    future_dates = [col for col in s_retcol if pd.to_datetime(col) >= event_date]\n",
    "    \n",
    "    if len(future_dates) > 0:\n",
    "        res = future_dates[0] \n",
    "        return row[res]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def nday_ret(row, days):\n",
    "    event_date = row['eventDate']\n",
    "    future_dates = [col for col in s_retcol if pd.to_datetime(col) >= event_date]\n",
    "\n",
    "    if len(future_dates) >= days:\n",
    "        res = [row[col] for col in future_dates[:days]]\n",
    "        cumres = (1 + pd.Series(res)).prod() - 1  \n",
    "        return cumres\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "    \n",
    "df_merge['eventDate'] = pd.to_datetime(df_merge['eventDate'])\n",
    "retcol = [col for col in df_merge.columns if col[:4].isdigit()]\n",
    "retdates = pd.to_datetime(retcol)\n",
    "s_retcol = [col for _, col in sorted(zip(retdates, retcol))]\n",
    "\n",
    "df_merge['fow_1d'] = df_merge.apply(oneday_ret, axis=1)\n",
    "df_merge['fow_3d'] = df_merge.apply(lambda row: nday_ret(row, 3), axis=1)\n",
    "df_merge['fow_7d'] = df_merge.apply(lambda row: nday_ret(row, 7), axis=1)\n",
    "\n",
    "df_merge[['fow_1d', 'fow_3d', 'fow_7d']] = df_merge[['fow_1d', 'fow_3d', 'fow_7d']].fillna(0)\n",
    "df_merge.head()\n",
    "\n",
    "# binarize into positive return or not\n",
    "\n",
    "df_merge['fow_1d_bin'] = df_merge['fow_1d'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df_merge['fow_3d_bin'] = df_merge['fow_3d'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df_merge['fow_7d_bin'] = df_merge['fow_7d'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2078a2",
   "metadata": {},
   "source": [
    "Given that we are looking at a dataset with many features and also happens to be very sparse, there are a few steps we should take.\n",
    "- We start by removing highly correlated features as a best practice.\n",
    "- Next, as a blanket approach to filtering sparsely populated features, we can filter by each feature's variance below a threshold, removing features that have little change and therefore little contribution.\n",
    "- Scale the independent variables.\n",
    "\n",
    "There are a few models to try with sparse data:\n",
    "- Linear Regression as a baseline check\n",
    "- Lasso Regularization to penalize certain features\n",
    "- RandomForest and XGBoost, as decision trees are well-equipped for sparse data\n",
    "\n",
    "We'll evaluate the performance of each model by looking at their R^2 value, and attempt to identify reoccurring features with high importance to see if there are any that can be used to predict postitive forward returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b21b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Qna Deception keyDriver positiveScore', 'Total Exec Change keyDriver positiveScore', 'Answer Exec Change keyDriver positiveScore', 'Answer Deception keyDriver negativeScore', 'Answer Guidance keyDriver positiveScore', 'Total Capital Raise Returns keyDriver positiveScore', 'Answer Headwinds Tailwinds keyDriver negativeScore', 'Total Exec Change keyDriver negativeScore', 'Total Wage keyDriver score'}\n"
     ]
    }
   ],
   "source": [
    "# remove correlated features\n",
    "\n",
    "drive_col = [col for col in df_merge.columns if 'keydriver' in col.lower()]\n",
    "\n",
    "df_corr = df_merge[drive_col]\n",
    "mat = df_corr.corr()\n",
    "\n",
    "to_remove = set()\n",
    "for i in range(len(mat.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(mat.iloc[i, j]) > 0.9:\n",
    "            colname = mat.columns[i]\n",
    "            to_remove.add(colname)\n",
    "\n",
    "print(to_remove)\n",
    "\n",
    "df_tpp = df_merge[drive_col].drop(columns=to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb06c87d",
   "metadata": {},
   "source": [
    "- Due to there being many features, some features may be very sparse and be mostly 0s. In that event, we filter and remove the features with low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e0797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=0.01)  # Remove features with very low variance\n",
    "df_reduced = df_tpp.iloc[:, selector.fit(df_tpp).get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f468d6",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa7041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reduced\n",
    "feature_names = df_reduced.columns\n",
    "\n",
    "#y1d = df_merge['fow_1d']\n",
    "#y3d = df_merge['fow_3d']\n",
    "#y7d = df_merge['fow_7d']\n",
    "\n",
    "y1d = df_merge['fow_1d_bin']\n",
    "y3d = df_merge['fow_3d_bin']\n",
    "y7d = df_merge['fow_7d_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc746b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Day Forward Return Linear Regression\n",
      "R^2: -0.21224036354801634\n",
      "MSE: 0.30305084025085477\n",
      "                                     Feature  Importance\n",
      "136  Presentation Irregularities keyDrive...    1.289802\n",
      "130     Total Irregularities keyDriver score    1.115469\n",
      "139  Answer Merger Acquisition keyDriver ...    0.694026\n",
      "137  Presentation Irregularities keyDrive...    0.621898\n",
      "131  Total Irregularities keyDriver negat...    0.536375\n",
      "145          Qna Exec Change keyDriver score    0.490342\n",
      "146  Qna Exec Change keyDriver negativeScore    0.483078\n",
      "138  Presentation Irregularities keyDrive...    0.447071\n",
      "133    Answer Irregularities keyDriver score    0.432729\n",
      "124  Answer Capital Raise Returns keyDriv...    0.385358\n",
      " \n",
      "3-Day Forward Return Linear Regression\n",
      "R^2: -0.5255556743718135\n",
      "MSE: 0.3598636870530623\n",
      "                                     Feature  Importance\n",
      "145          Qna Exec Change keyDriver score    1.323129\n",
      "139  Answer Merger Acquisition keyDriver ...    1.227862\n",
      "133    Answer Irregularities keyDriver score    1.031828\n",
      "146  Qna Exec Change keyDriver negativeScore    0.835097\n",
      "135  Answer Irregularities keyDriver posi...    0.791380\n",
      "147  Qna Exec Change keyDriver positiveScore    0.567497\n",
      "142  Question Irregularities keyDriver score    0.565293\n",
      "127       Qna Irregularities keyDriver score    0.524743\n",
      "140  Answer Merger Acquisition keyDriver ...    0.505078\n",
      "143  Question Irregularities keyDriver ne...    0.446560\n",
      " \n",
      "5-Day Forward Return Linear Regression\n",
      "R^2: -0.3829553723353947\n",
      "MSE: 0.28637615597580396\n",
      "                                     Feature  Importance\n",
      "145          Qna Exec Change keyDriver score    1.406079\n",
      "133    Answer Irregularities keyDriver score    1.163594\n",
      "146  Qna Exec Change keyDriver negativeScore    0.991046\n",
      "135  Answer Irregularities keyDriver posi...    0.644708\n",
      "134  Answer Irregularities keyDriver nega...    0.614579\n",
      "139  Answer Merger Acquisition keyDriver ...    0.596427\n",
      "125  Answer Capital Raise Returns keyDriv...    0.466832\n",
      "147  Qna Exec Change keyDriver positiveScore    0.450984\n",
      "141  Answer Merger Acquisition keyDriver ...    0.403433\n",
      "137  Presentation Irregularities keyDrive...    0.400318\n",
      " \n"
     ]
    }
   ],
   "source": [
    "forwards = [(1, y1d), (3, y3d), (5, y7d)]\n",
    "\n",
    "for num, table in forwards:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, table, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    feature_importance = np.abs(model.coef_)\n",
    "    df_feat = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    df_feat = df_feat.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "\n",
    "    print(f\"{num}-Day Forward Return Linear Regression\")\n",
    "    print(\"R^2:\", r2)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(df_feat.head(10))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf580e9",
   "metadata": {},
   "source": [
    "# Lasso Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27dcc4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Day Forward Return LassoCV\n",
      "R^2: 0.02413155549624013\n",
      "MSE: 0.24395966424974183\n",
      "                                     Feature  Importance\n",
      "10     Qna Deception keyDriver negativeScore    0.003581\n",
      "29          Answer Deception keyDriver score    0.000036\n",
      "31   Question Capital Raise Returns keyDr...    0.000027\n",
      "97   Presentation Headwinds Tailwinds key...    0.000000\n",
      "98   Presentation Headwinds Tailwinds key...    0.000000\n",
      "99    Answer Market Position keyDriver score    0.000000\n",
      "100  Answer Market Position keyDriver neg...    0.000000\n",
      "101  Answer Market Position keyDriver pos...    0.000000\n",
      "102          Question Margin keyDriver score    0.000000\n",
      "103  Question Margin keyDriver negativeScore    0.000000\n",
      " \n",
      "3-Day Forward Return LassoCV\n",
      "R^2: -0.015915358271915148\n",
      "MSE: 0.23964451294909678\n",
      "                                     Feature  Importance\n",
      "36   Presentation Wage keyDriver positive...    0.006740\n",
      "66   Total Headwinds Tailwinds keyDriver ...    0.005899\n",
      "65   Total Headwinds Tailwinds keyDriver ...    0.004715\n",
      "10     Qna Deception keyDriver negativeScore    0.003642\n",
      "18   Total Merger Acquisition keyDriver p...    0.003199\n",
      "14   Total Deception keyDriver negativeScore    0.001934\n",
      "29          Answer Deception keyDriver score    0.000027\n",
      "31   Question Capital Raise Returns keyDr...    0.000025\n",
      "0                   Qna Wage keyDriver score    0.000000\n",
      "100  Answer Market Position keyDriver neg...    0.000000\n",
      " \n",
      "5-Day Forward Return LassoCV\n",
      "R^2: -0.005223245264588838\n",
      "MSE: 0.20815709214843767\n",
      "                                     Feature  Importance\n",
      "29          Answer Deception keyDriver score    0.000044\n",
      "31   Question Capital Raise Returns keyDr...    0.000024\n",
      "104  Question Margin keyDriver positiveScore    0.000000\n",
      "97   Presentation Headwinds Tailwinds key...    0.000000\n",
      "98   Presentation Headwinds Tailwinds key...    0.000000\n",
      "99    Answer Market Position keyDriver score    0.000000\n",
      "100  Answer Market Position keyDriver neg...    0.000000\n",
      "101  Answer Market Position keyDriver pos...    0.000000\n",
      "102          Question Margin keyDriver score    0.000000\n",
      "103  Question Margin keyDriver negativeScore    0.000000\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for num, table in forwards:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, table, test_size=0.2, random_state=42)\n",
    "    \n",
    "    param_grid = {'alpha': np.logspace(-4, 1, 50)} \n",
    "    \n",
    "    lasso = Lasso()\n",
    "    \n",
    "    grid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_alpha = grid_search.best_params_['alpha']\n",
    "\n",
    "    lasso_best = Lasso(alpha=best_alpha)\n",
    "    lasso_best.fit(X_train, y_train)\n",
    "    y_pred = lasso_best.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    feature_importance = np.abs(lasso_best.coef_)\n",
    "\n",
    "    df_las_imp = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    df_las_imp = df_las_imp.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    print(f\"{num}-Day Forward Return LassoCV\")\n",
    "    print(\"R^2:\", r2)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(df_las_imp.head(10))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923aef3a",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cba26e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Day Forward Return Random Forest\n",
      "R^2: -0.05129175824175847\n",
      "MSE: 0.26281491712707183\n",
      "                                    Feature  Importance\n",
      "9             Qna Deception keyDriver score    0.030538\n",
      "19           Total Guidance keyDriver score    0.025018\n",
      "22  Total Capital Raise Returns keyDrive...    0.021717\n",
      "16  Total Merger Acquisition keyDriver s...    0.020661\n",
      "29         Answer Deception keyDriver score    0.020611\n",
      "27          Answer Guidance keyDriver score    0.019777\n",
      "37   Presentation Deception keyDriver score    0.019405\n",
      "52      Qna Market Position keyDriver score    0.019033\n",
      "87    Presentation Guidance keyDriver score    0.018814\n",
      "96  Presentation Headwinds Tailwinds key...    0.018164\n",
      " \n",
      "3-Day Forward Return Random Forest\n",
      "R^2: -0.06820681935817796\n",
      "MSE: 0.2519795580110497\n",
      "                                    Feature  Importance\n",
      "9             Qna Deception keyDriver score    0.026530\n",
      "10    Qna Deception keyDriver negativeScore    0.023715\n",
      "61    Total Market Position keyDriver score    0.023457\n",
      "64  Total Headwinds Tailwinds keyDriver ...    0.022962\n",
      "96  Presentation Headwinds Tailwinds key...    0.021703\n",
      "22  Total Capital Raise Returns keyDrive...    0.019802\n",
      "19           Total Guidance keyDriver score    0.019338\n",
      "34        Presentation Wage keyDriver score    0.018937\n",
      "46                Qna CapEx keyDriver score    0.018712\n",
      "27          Answer Guidance keyDriver score    0.018705\n",
      " \n",
      "5-Day Forward Return Random Forest\n",
      "R^2: -0.051718322523585325\n",
      "MSE: 0.21778508287292817\n",
      "                                    Feature  Importance\n",
      "27          Answer Guidance keyDriver score    0.030829\n",
      "96  Presentation Headwinds Tailwinds key...    0.024310\n",
      "64  Total Headwinds Tailwinds keyDriver ...    0.021355\n",
      "29         Answer Deception keyDriver score    0.020693\n",
      "3              Qna Guidance keyDriver score    0.019774\n",
      "9             Qna Deception keyDriver score    0.019415\n",
      "31  Question Capital Raise Returns keyDr...    0.019271\n",
      "16  Total Merger Acquisition keyDriver s...    0.018800\n",
      "19           Total Guidance keyDriver score    0.018421\n",
      "22  Total Capital Raise Returns keyDrive...    0.018031\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for num, table in forwards:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, table, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    df_grad_imp = pd.DataFrame({\"Feature\": X.columns, \"Importance\": rf.feature_importances_})\n",
    "    df_grad_imp = df_grad_imp.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    print(f\"{num}-Day Forward Return Random Forest\")\n",
    "    print(\"R^2:\", r2)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(df_grad_imp.head(10))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9ebf6",
   "metadata": {},
   "source": [
    "## XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90e81852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "1-Day Forward Return LassoCV\n",
      "R^2: -0.008499583855449577\n",
      "MSE: 0.2521172000786341\n",
      "                                     Feature  Importance\n",
      "107  Question Headwinds Tailwinds keyDriv...    0.014660\n",
      "112     Answer CapEx keyDriver negativeScore    0.012348\n",
      "39   Presentation Deception keyDriver pos...    0.011403\n",
      "117        Total Exec Change keyDriver score    0.010794\n",
      "37    Presentation Deception keyDriver score    0.010672\n",
      "54   Qna Market Position keyDriver positi...    0.010520\n",
      "47         Qna CapEx keyDriver negativeScore    0.010519\n",
      "96   Presentation Headwinds Tailwinds key...    0.010306\n",
      "83   Presentation CapEx keyDriver positiv...    0.010299\n",
      "52       Qna Market Position keyDriver score    0.010230\n",
      " \n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "3-Day Forward Return LassoCV\n",
      "R^2: -0.052880535815356566\n",
      "MSE: 0.24836423737923372\n",
      "                                     Feature  Importance\n",
      "106  Question Headwinds Tailwinds keyDriv...    0.010688\n",
      "30   Answer Deception keyDriver positiveS...    0.009308\n",
      "39   Presentation Deception keyDriver pos...    0.009302\n",
      "120  Question Merger Acquisition keyDrive...    0.009234\n",
      "119  Question Merger Acquisition keyDrive...    0.009126\n",
      "95   Presentation Capital Raise Returns k...    0.009043\n",
      "62   Total Market Position keyDriver nega...    0.009026\n",
      "122  Presentation Exec Change keyDriver n...    0.008993\n",
      "14   Total Deception keyDriver negativeScore    0.008854\n",
      "96   Presentation Headwinds Tailwinds key...    0.008813\n",
      " \n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "5-Day Forward Return LassoCV\n",
      "R^2: -0.011655558543950395\n",
      "MSE: 0.209489066547485\n",
      "                                     Feature  Importance\n",
      "56       Total CapEx keyDriver negativeScore    0.015138\n",
      "57       Total CapEx keyDriver positiveScore    0.014905\n",
      "43                Qna Margin keyDriver score    0.013694\n",
      "45        Qna Margin keyDriver positiveScore    0.013383\n",
      "99    Answer Market Position keyDriver score    0.013108\n",
      "17   Total Merger Acquisition keyDriver n...    0.013046\n",
      "84       Presentation Margin keyDriver score    0.013036\n",
      "46                 Qna CapEx keyDriver score    0.012995\n",
      "65   Total Headwinds Tailwinds keyDriver ...    0.012912\n",
      "109  Question Guidance keyDriver negative...    0.012886\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for num, table in forwards:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, table, test_size=0.2, random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"subsample\": [0.8, 1.0]}\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring=\"r2\", verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_xgb = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "    r2_xgb = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    df_xgb_imp = pd.DataFrame({\"Feature\": X.columns, \"Importance\": best_xgb.feature_importances_})\n",
    "    df_xgb_imp = df_xgb_imp.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    print(f\"{num}-Day Forward Return LassoCV\")\n",
    "    print(\"R^2:\", r2_xgb)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(df_xgb_imp.head(10))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53768249",
   "metadata": {},
   "source": [
    "## Part 2 Reflection\n",
    "\n",
    "\n",
    "- Traditional machine learning models ineffective at capturing the relationship between keydriver feature scores and forward returns, demonstrated by the low R^2 values across all models.\n",
    "- At best, the LassoCV regularization model returns positive R^2 values for the single-day forward return range.\n",
    "- The overall keyDriver feature importance is very low for most models, with the only exceptions being seen in the Linear Regression model, with some of the most prevalent being 'Presentation Irregularities', and 'QNA Exec Change'\n",
    "- If I had more time or started from scratch, I would experiment with approaches using deep learning models, which may be able to find the relationship between features and returns more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9273aa",
   "metadata": {},
   "source": [
    "# 3. Modeling forward stock returns with constructedcategories\n",
    "\n",
    "- For this section, I organize, preprocess, and evaluate the data with a variety of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eed3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter irrelevant data as instructed\n",
    "\n",
    "df_data = df_data_temp.copy()\n",
    "df_data = df_data[df_data['documentType'] == 'EARNINGS_CALL']\n",
    "df_data = df_data[df_data['country'] == 'US']\n",
    "\n",
    "# Forward fill any missing identifiers by referencing earnings calls with the same id\n",
    "# Remove event columns as instructed\n",
    "# Populate nan keydriver cells with neutral score of 0\n",
    "# Populate nan Method2 Category cells with neutral scores of [0,0]\n",
    "\n",
    "df_data = df_data.sort_values(by=['companyId', 'mainIdentifier isin'])\n",
    "df_data['mainIdentifier isin'] = df_data.groupby(['companyId', 'companyName'])['mainIdentifier isin'].transform(lambda x: x.ffill())\n",
    "df_data = df_data.rename(columns={'mainIdentifier isin': 'fsym_isin'})\n",
    "\n",
    "event_col = [col for col in df_data.columns if 'eventcount' in col.lower()]\n",
    "df_data = df_data.drop(columns = event_col)\n",
    "\n",
    "score_col = [col for col in df_data.columns if 'score' in col.lower()]\n",
    "df_data = df_data.drop(columns = score_col)\n",
    "\n",
    "df_data.iloc[:, 28:]\n",
    "df_data.iloc[:, 28:] = df_data.iloc[:, 28:].fillna('[0, 0]')\n",
    "\n",
    "# Group Method2 Category by prefix\n",
    "# Calculate the aggregate [positive,negative] score for each Category in a dynamically created column\n",
    "\n",
    "for prefix, cols in grouped_cols.items():\n",
    "    new_col_name = f\"{prefix}_Total\"\n",
    "    df_data[new_col_name] = df_data.apply(lambda row: sum_tuple_group(row, cols), axis=1)\n",
    "    \n",
    "cats = list(df_data.iloc[:,28:].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534fe31",
   "metadata": {},
   "source": [
    "- Since [0,0] is a very inconvenient string to work with, I create two new columns dynamically\n",
    "- One column will be the positive score while the other depicts the negative score.\n",
    "- Then merge with the returns dataset to filter out earnings without returns or unassociated returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1996583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_list(value):\n",
    "    try:\n",
    "        if isinstance(value, str) and value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "            return literal_eval(value)\n",
    "        else:\n",
    "            return [0, 0]  \n",
    "    except (ValueError, SyntaxError):\n",
    "        return [0, 0]  \n",
    "\n",
    "\n",
    "for col in cats:\n",
    "    df_data[col] = df_data[col].apply(safe_parse_list)\n",
    "    \n",
    "    df_data[[f\"{col} pos\", f\"{col} neg\"]] = pd.DataFrame(df_data[col].tolist(), index=df_data.index)\n",
    "    df_data.drop(columns=[col], inplace=True)\n",
    "\n",
    "    \n",
    "df_merge3 = pd.merge(df_data, df_retwide, on=['fsym_isin'], how='inner')\n",
    "\n",
    "df_inputs3 = df_merge3.iloc[:,28:770]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1b083",
   "metadata": {},
   "source": [
    "- Use correlation matrix to remove highly correlated variables, although the quantity of these is very low. \n",
    "- Since these categories are very sparse, the variance threshold is effective at removing the majority of catergories that are mostly 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74ec7a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Employment - Hypothetical neg', 'Spin Off Split Off neg'}\n"
     ]
    }
   ],
   "source": [
    "mat = df_inputs3.corr()\n",
    "\n",
    "to_remove = set()\n",
    "for i in range(len(mat.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(mat.iloc[i, j]) > 0.9:\n",
    "            colname = mat.columns[i]\n",
    "            to_remove.add(colname)\n",
    "\n",
    "print(to_remove)\n",
    "\n",
    "df_tepp = df_inputs3.drop(columns=to_remove)\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)  # Remove features with very low variance\n",
    "df_rduced = df_tepp.iloc[:, selector.fit(df_tepp).get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4a4d3",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d3494d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Day Forward Return Linear Regression\n",
      "R^2: -0.26443322158008\n",
      "MSE: 0.31609865647388213\n",
      "                                  Feature  Importance\n",
      "225             Product Trial Results pos    0.485794\n",
      "236             Settlement - Question neg    0.439078\n",
      "212                        Employment neg    0.411603\n",
      "240               Dividend - Question pos    0.325062\n",
      "216  Customer Spending - Hypothetical neg    0.308125\n",
      "73                          Tailwinds neg    0.280274\n",
      "196            Writedowns Impairments neg    0.239461\n",
      "217   Merger Acquisition Announcement pos    0.236729\n",
      "172      Financial Results - Question neg    0.204233\n",
      "181                Volume - Qualifier neg    0.202834\n",
      " \n",
      "3-Day Forward Return Linear Regression\n",
      "R^2: -0.56758924101893\n",
      "MSE: 0.36977899498166394\n",
      "                                  Feature  Importance\n",
      "225             Product Trial Results pos    0.346474\n",
      "216  Customer Spending - Hypothetical neg    0.339190\n",
      "227         Debt Financing - Forecast pos    0.331088\n",
      "221                  Product Approval pos    0.317388\n",
      "150                       Competition pos    0.307625\n",
      "208                           Lawsuit neg    0.306906\n",
      "67                 Strategic Alliance neg    0.301037\n",
      "154    Commodity Price - Hypothetical pos    0.272337\n",
      "243            Facilities - Qualifier neg    0.262748\n",
      "212                        Employment neg    0.236738\n",
      " \n",
      "5-Day Forward Return Linear Regression\n",
      "R^2: -0.9583001858064479\n",
      "MSE: 0.4055159628982917\n",
      "                                   Feature  Importance\n",
      "244                   Catastrophe Loss neg    0.556881\n",
      "227          Debt Financing - Forecast pos    0.539186\n",
      "215        Customer Traffic - Question neg    0.387171\n",
      "212                         Employment neg    0.379476\n",
      "243             Facilities - Qualifier neg    0.312463\n",
      "223  Merger Acquisition - Hypothetical neg    0.309424\n",
      "235              Settlement - Question pos    0.303110\n",
      "154     Commodity Price - Hypothetical pos    0.289330\n",
      "137       Margin Commentary - Question pos    0.268737\n",
      "221                   Product Approval pos    0.266522\n",
      " \n"
     ]
    }
   ],
   "source": [
    "X = df_rduced\n",
    "feature_names = df_rduced.columns\n",
    "\n",
    "#y1d = df_merge['fow_1d']\n",
    "#y3d = df_merge['fow_3d']\n",
    "#y7d = df_merge['fow_7d']\n",
    "\n",
    "y1d = df_merge['fow_1d_bin']\n",
    "y3d = df_merge['fow_3d_bin']\n",
    "y7d = df_merge['fow_7d_bin']\n",
    "\n",
    "for num, table in forwards:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, table, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    feature_importance = np.abs(model.coef_)\n",
    "    df_feat = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    df_feat = df_feat.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "\n",
    "    print(f\"{num}-Day Forward Return Linear Regression\")\n",
    "    print(\"R^2:\", r2)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(df_feat.head(10))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6025e66",
   "metadata": {},
   "source": [
    "## LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2854d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Day Forward Return LassoCV\n",
      "R^2: 0.0069752908721810725\n",
      "MSE: 0.2482485994858776\n",
      "                               Feature  Importance\n",
      "33  Business Commentary - Forecast neg    0.009162\n",
      "2             Financial Commentary pos    0.009026\n",
      "3             Financial Commentary neg    0.008282\n",
      "15            AmenityQuestionTopic neg    0.006433\n",
      "7                Financial Results neg    0.006395\n",
      "42             Category Commentary pos    0.004207\n",
      "9                        Euphemism neg    0.002101\n",
      "26               Margin Commentary pos    0.002028\n",
      "17             Business Commentary neg    0.001393\n",
      "16             Business Commentary pos    0.000853\n",
      " \n",
      "3-Day Forward Return LassoCV\n",
      "R^2: -0.015382980445472594\n",
      "MSE: 0.23951893021832701\n",
      "                                  Feature  Importance\n",
      "115                           Weather neg    0.031161\n",
      "58                Capacity Production pos    0.028664\n",
      "80                   Price - Question pos    0.020368\n",
      "5                          Facilities neg    0.017516\n",
      "17                Business Commentary neg    0.015964\n",
      "8                           Euphemism pos    0.014641\n",
      "64                   Price - Forecast pos    0.013334\n",
      "68   Financial Commentary - Qualifier pos    0.012982\n",
      "72                          Tailwinds pos    0.012699\n",
      "19    Financial Commentary - Forecast neg    0.012107\n",
      " \n",
      "5-Day Forward Return LassoCV\n",
      "R^2: -0.0040628685141519405\n",
      "MSE: 0.20791680656878614\n",
      "                                Feature  Importance\n",
      "0                             Price pos         0.0\n",
      "154  Commodity Price - Hypothetical pos         0.0\n",
      "156            Price - Hypothetical neg         0.0\n",
      "157             Cost - Hypothetical pos         0.0\n",
      "158             Cost - Hypothetical neg         0.0\n",
      "159                  Consumer Trend pos         0.0\n",
      "160                  Consumer Trend neg         0.0\n",
      "161                  Tax - Forecast pos         0.0\n",
      "162                  Tax - Forecast neg         0.0\n",
      "163                  Tax - Question neg         0.0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for num, table in forwards:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, table, test_size=0.2, random_state=42)\n",
    "    \n",
    "    param_grid = {'alpha': np.logspace(-4, 1, 50)} \n",
    "    \n",
    "    lasso = Lasso()\n",
    "    \n",
    "    grid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_alpha = grid_search.best_params_['alpha']\n",
    "\n",
    "    lasso_best = Lasso(alpha=best_alpha)\n",
    "    lasso_best.fit(X_train, y_train)\n",
    "    y_pred = lasso_best.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    feature_importance = np.abs(lasso_best.coef_)\n",
    "\n",
    "    df_las_imp = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    df_las_imp = df_las_imp.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    print(f\"{num}-Day Forward Return LassoCV\")\n",
    "    print(\"R^2:\", r2)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(df_las_imp.head(10))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa464238",
   "metadata": {},
   "source": [
    "## Part 3 Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63be89",
   "metadata": {},
   "source": [
    "- Just as in Part 2, traditional machine learning models experience difficulty in finding the relationship between features and postive returns, with the LassoCV regularization model offering the best R^2 value for a single-day future.\n",
    "- Regardless, the linear model identified some of the most important features to be 'Product Trials - Positive', 'Customer Spending - Negative', and 'Catastrophic Loss - Negative'.\n",
    "- Contextually, these features make sense. A successful product trial, being the cornerstone of a company is an essential part of success. Meanwhile, the latter two cases highlight the ramifications of two negative occurrences to a company's earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5977e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
